import requests
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
import json
from datetime import datetime
import time

def translate_to_japanese(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
    try:
        if not text or len(text) > 5000:
            return text
        translator = GoogleTranslator(source='auto', target='ja')
        time.sleep(0.5)  # ç¿»è¨³APIåˆ¶é™å¯¾ç­–
        return translator.translate(text)
    except Exception as e:
        print(f"  âœ— ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
        return text

def collect_bbc_myanmar():
    """BBC Myanmar ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    news_list = []
    try:
        print("  â†’ BBC Myanmar ã‹ã‚‰åé›†ä¸­...")
        url = "https://www.bbc.com/news/topics/c8nq32jw5r7t"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = soup.find_all(['div', 'article'], limit=10)
        
        for article in articles:
            try:
                title_tag = article.find(['h2', 'h3', 'a'])
                if title_tag:
                    title = title_tag.get_text(strip=True)
                    if len(title) < 10 or 'Myanmar' not in title and 'Burma' not in title:
                        continue
                    
                    link_tag = article.find('a', href=True)
                    link = link_tag['href'] if link_tag else ""
                    if link and not link.startswith('http'):
                        link = "https://www.bbc.com" + link
                    
                    if title and link:
                        news_list.append({
                            'title': title,
                            'title_ja': translate_to_japanese(title),
                            'url': link,
                            'source': 'BBC News',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        print(f"  âœ“ BBC: {title[:50]}...")
            except Exception as e:
                continue
        
        print(f"  âœ“ BBC: {len(news_list)}ä»¶åé›†")
    except Exception as e:
        print(f"  âœ— BBCã‚¨ãƒ©ãƒ¼: {e}")
    
    return news_list

def collect_reuters_myanmar():
    """Reuters Myanmar ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    news_list = []
    try:
        print("  â†’ Reuters Myanmar ã‹ã‚‰åé›†ä¸­...")
        url = "https://www.reuters.com/world/asia-pacific/myanmar/"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = soup.find_all('a', {'data-testid': 'Heading'}, limit=10)
        
        for article in articles:
            try:
                title = article.get_text(strip=True)
                if len(title) < 10:
                    continue
                    
                link = article.get('href', '')
                if link and not link.startswith('http'):
                    link = "https://www.reuters.com" + link
                
                if title and link:
                    news_list.append({
                        'title': title,
                        'title_ja': translate_to_japanese(title),
                        'url': link,
                        'source': 'Reuters',
                        'date': datetime.now().strftime('%Y-%m-%d')
                    })
                    print(f"  âœ“ Reuters: {title[:50]}...")
            except Exception as e:
                continue
        
        print(f"  âœ“ Reuters: {len(news_list)}ä»¶åé›†")
    except Exception as e:
        print(f"  âœ— Reutersã‚¨ãƒ©ãƒ¼: {e}")
    
    return news_list

def collect_rfa_myanmar():
    """Radio Free Asia Myanmar ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    news_list = []
    try:
        print("  â†’ Radio Free Asia Myanmar ã‹ã‚‰åé›†ä¸­...")
        url = "https://www.rfa.org/english/news/myanmar"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=15)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        articles = soup.find_all(['h2', 'h3', 'div'], class_=['story', 'title'], limit=10)
        
        for article in articles:
            try:
                title_tag = article.find('a') or article
                title = title_tag.get_text(strip=True)
                
                if len(title) < 10:
                    continue
                
                link_tag = article.find('a', href=True)
                link = link_tag['href'] if link_tag else ""
                if link and not link.startswith('http'):
                    link = "https://www.rfa.org" + link
                
                if title and link:
                    news_list.append({
                        'title': title,
                        'title_ja': translate_to_japanese(title),
                        'url': link,
                        'source': 'Radio Free Asia',
                        'date': datetime.now().strftime('%Y-%m-%d')
                    })
                    print(f"  âœ“ RFA: {title[:50]}...")
            except Exception as e:
                continue
        
        print(f"  âœ“ RFA: {len(news_list)}ä»¶åé›†")
    except Exception as e:
        print(f"  âœ— RFAã‚¨ãƒ©ãƒ¼: {e}")
    
    return news_list

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸ“° ãƒŸãƒ£ãƒ³ãƒãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚’é–‹å§‹...")
    print("=" * 50)
    
    all_news = []
    
    # å„ã‚µã‚¤ãƒˆã‹ã‚‰åé›†
    all_news.extend(collect_bbc_myanmar())
    all_news.extend(collect_reuters_myanmar())
    all_news.extend(collect_rfa_myanmar())
    
    print("=" * 50)
    print(f"ğŸ“Š åˆè¨ˆ: {len(all_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†")
    
    # é‡è¤‡ã‚’å‰Šé™¤ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãŒåŒã˜ã‚‚ã®ã‚’é™¤å¤–ï¼‰
    seen_titles = set()
    unique_news = []
    for news in all_news:
        if news['title'] not in seen_titles:
            seen_titles.add(news['title'])
            unique_news.append(news)
    
    # ãƒˆãƒƒãƒ—10ã‚’é¸æŠ
    top_10 = unique_news[:10]
    
    # JSONã«ä¿å­˜
    with open('myanmar_news_top10.json', 'w', encoding='utf-8') as f:
        json.dump(top_10, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å®Œäº†ï¼{len(top_10)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    print("=" * 50)
    
    # ä¿å­˜ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¡¨ç¤º
    for i, news in enumerate(top_10, 1):
        print(f"{i}. [{news['source']}] {news['title_ja'][:60]}...")

if __name__ == "__main__":
    main()
