# -*- coding: utf-8 -*-
"""
Myanmar News Collector PRO
20ã‚µã‚¤ãƒˆã‹ã‚‰1æ—¥10ä»¶ã®å³é¸ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
from deep_translator import GoogleTranslator
import time
import random

class MyanmarNewsCollector:
    def __init__(self):
        self.translator = GoogleTranslator(source='auto', target='ja')
        self.news_sources = {
            # æ—¥æœ¬
            'NHK': 'https://www3.nhk.or.jp/news/word/0000178.html',
            'æ—¥çµŒ': 'https://www.nikkei.com/search?keyword=ãƒŸãƒ£ãƒ³ãƒãƒ¼',
            
            # ã‚¢ãƒ¡ãƒªã‚«
            'CNN': 'https://edition.cnn.com/search?q=myanmar',
            'NYT': 'https://www.nytimes.com/search?query=myanmar',
            'RFA': 'https://www.rfa.org/english/news/myanmar',
            
            # ã‚¤ã‚®ãƒªã‚¹
            'BBC': 'https://www.bbc.com/search?q=myanmar',
            'Guardian': 'https://www.theguardian.com/world/myanmar',
            'Reuters': 'https://www.reuters.com/world/asia-pacific/myanmar/',
            
            # ä¸­å›½
            'Xinhua': 'http://www.xinhuanet.com/english/search.htm?searchWord=myanmar',
            'CGTN': 'https://www.cgtn.com/search/myanmar',
            
            # ã‚¤ãƒ³ãƒ‰
            'Times of India': 'https://timesofindia.indiatimes.com/topic/myanmar',
            'Hindustan': 'https://www.hindustantimes.com/topic/myanmar',
            
            # ã‚¿ã‚¤
            'Bangkok Post': 'https://www.bangkokpost.com/search?q=myanmar',
            'Nation': 'https://www.nationthailand.com/search?q=myanmar',
            
            # ã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«
            'Straits Times': 'https://www.straitstimes.com/search/myanmar',
            'CNA': 'https://www.channelnewsasia.com/search?q=myanmar',
            
            # å›½éš›æ©Ÿé–¢
            'UN News': 'https://news.un.org/en/tags/myanmar',
            'Al Jazeera': 'https://www.aljazeera.com/search/myanmar',
            'VOA': 'https://www.voanews.com/search?q=myanmar',
            'Irrawaddy': 'https://www.irrawaddy.com/'
        }
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        self.all_news = []
    
    def translate_text(self, text, max_retries=3):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³"""
        if not text:
            return ""
        
        for attempt in range(max_retries):
            try:
                # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ†å‰²
                if len(text) > 5000:
                    text = text[:5000]
                
                translated = self.translator.translate(text)
                time.sleep(0.5)  # APIåˆ¶é™å¯¾ç­–
                return translated
            except Exception as e:
                if attempt < max_retries - 1:
                    time.sleep(2)
                    continue
                else:
                    print(f"ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    return text
    
    def clean_text(self, text):
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not text:
            return ""
        return ' '.join(text.split()).strip()
    
    def fetch_bbc_news(self):
        """BBC ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        try:
            print("ğŸ“¡ BBC ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
            url = 'https://www.bbc.com/news/topics/c8nq32jwvg1t'
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all('div', {'data-testid': 'edinburgh-card'}, limit=3)
            
            for article in articles:
                try:
                    title_tag = article.find('h2')
                    link_tag = article.find('a', href=True)
                    desc_tag = article.find('p')
                    
                    if title_tag and link_tag:
                        title = self.clean_text(title_tag.get_text())
                        link = link_tag['href']
                        if not link.startswith('http'):
                            link = 'https://www.bbc.com' + link
                        
                        description = self.clean_text(desc_tag.get_text()) if desc_tag else ""
                        
                        # ç¿»è¨³
                        title_ja = self.translate_text(title)
                        desc_ja = self.translate_text(description)
                        
                        self.all_news.append({
                            'title': title_ja,
                            'description': desc_ja,
                            'source': 'BBC News',
                            'url': link,
                            'country': 'ã‚¤ã‚®ãƒªã‚¹',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        print(f"  âœ“ {title_ja[:50]}...")
                except:
                    continue
            
            print(f"  âœ… BBC: {len([n for n in self.all_news if n['source']=='BBC News'])}ä»¶å–å¾—\n")
        except Exception as e:
            print(f"  âŒ BBC ã‚¨ãƒ©ãƒ¼: {str(e)}\n")
    
    def fetch_reuters_news(self):
        """Reuters ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        try:
            print("ğŸ“¡ Reuters ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
            url = 'https://www.reuters.com/world/asia-pacific/myanmar/'
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all('li', {'data-testid': 'Body'}, limit=3)
            
            for article in articles:
                try:
                    link_tag = article.find('a', href=True)
                    title_tag = article.find('h3')
                    
                    if link_tag and title_tag:
                        title = self.clean_text(title_tag.get_text())
                        link = link_tag['href']
                        if not link.startswith('http'):
                            link = 'https://www.reuters.com' + link
                        
                        # ç¿»è¨³
                        title_ja = self.translate_text(title)
                        
                        self.all_news.append({
                            'title': title_ja,
                            'description': '',
                            'source': 'Reuters',
                            'url': link,
                            'country': 'ã‚¤ã‚®ãƒªã‚¹',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        print(f"  âœ“ {title_ja[:50]}...")
                except:
                    continue
            
            print(f"  âœ… Reuters: {len([n for n in self.all_news if n['source']=='Reuters'])}ä»¶å–å¾—\n")
        except Exception as e:
            print(f"  âŒ Reuters ã‚¨ãƒ©ãƒ¼: {str(e)}\n")
    
    def fetch_rfa_news(self):
        """Radio Free Asia ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        try:
            print("ğŸ“¡ Radio Free Asia ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
            url = 'https://www.rfa.org/english/news/myanmar'
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all('div', class_='sectionteaser', limit=3)
            
            for article in articles:
                try:
                    link_tag = article.find('a', href=True)
                    title_tag = article.find('h2')
                    desc_tag = article.find('p')
                    
                    if link_tag and title_tag:
                        title = self.clean_text(title_tag.get_text())
                        link = link_tag['href']
                        if not link.startswith('http'):
                            link = 'https://www.rfa.org' + link
                        
                        description = self.clean_text(desc_tag.get_text()) if desc_tag else ""
                        
                        # ç¿»è¨³
                        title_ja = self.translate_text(title)
                        desc_ja = self.translate_text(description)
                        
                        self.all_news.append({
                            'title': title_ja,
                            'description': desc_ja,
                            'source': 'Radio Free Asia',
                            'url': link,
                            'country': 'ã‚¢ãƒ¡ãƒªã‚«',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        print(f"  âœ“ {title_ja[:50]}...")
                except:
                    continue
            
            print(f"  âœ… RFA: {len([n for n in self.all_news if n['source']=='Radio Free Asia'])}ä»¶å–å¾—\n")
        except Exception as e:
            print(f"  âŒ RFA ã‚¨ãƒ©ãƒ¼: {str(e)}\n")
    
    def fetch_aljazeera_news(self):
        """Al Jazeera ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        try:
            print("ğŸ“¡ Al Jazeera ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
            url = 'https://www.aljazeera.com/tag/myanmar/'
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all('article', limit=3)
            
            for article in articles:
                try:
                    link_tag = article.find('a', href=True)
                    title_tag = article.find('h3')
                    
                    if link_tag and title_tag:
                        title = self.clean_text(title_tag.get_text())
                        link = link_tag['href']
                        if not link.startswith('http'):
                            link = 'https://www.aljazeera.com' + link
                        
                        # ç¿»è¨³
                        title_ja = self.translate_text(title)
                        
                        self.all_news.append({
                            'title': title_ja,
                            'description': '',
                            'source': 'Al Jazeera',
                            'url': link,
                            'country': 'å›½éš›',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        print(f"  âœ“ {title_ja[:50]}...")
                except:
                    continue
            
            print(f"  âœ… Al Jazeera: {len([n for n in self.all_news if n['source']=='Al Jazeera'])}ä»¶å–å¾—\n")
        except Exception as e:
            print(f"  âŒ Al Jazeera ã‚¨ãƒ©ãƒ¼: {str(e)}\n")
    
    def fetch_irrawaddy_news(self):
        """The Irrawaddy ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        try:
            print("ğŸ“¡ The Irrawaddy ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
            url = 'https://www.irrawaddy.com/news'
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all('div', class_='article-content', limit=3)
            
            for article in articles:
                try:
                    link_tag = article.find('a', href=True)
                    title_tag = article.find('h3')
                    desc_tag = article.find('p')
                    
                    if link_tag and title_tag:
                        title = self.clean_text(title_tag.get_text())
                        link = link_tag['href']
                        description = self.clean_text(desc_tag.get_text()) if desc_tag else ""
                        
                        # ç¿»è¨³
                        title_ja = self.translate_text(title)
                        desc_ja = self.translate_text(description)
                        
                        self.all_news.append({
                            'title': title_ja,
                            'description': desc_ja,
                            'source': 'The Irrawaddy',
                            'url': link,
                            'country': 'ãƒŸãƒ£ãƒ³ãƒãƒ¼',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        print(f"  âœ“ {title_ja[:50]}...")
                except:
                    continue
            
            print(f"  âœ… Irrawaddy: {len([n for n in self.all_news if n['source']=='The Irrawaddy'])}ä»¶å–å¾—\n")
        except Exception as e:
            print(f"  âŒ Irrawaddy ã‚¨ãƒ©ãƒ¼: {str(e)}\n")
    
    def fetch_un_news(self):
        """UN News ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        try:
            print("ğŸ“¡ UN News ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
            url = 'https://news.un.org/en/tags/myanmar'
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all('div', class_='story-content', limit=3)
            
            for article in articles:
                try:
                    link_tag = article.find('a', href=True)
                    title_tag = article.find('h3')
                    
                    if link_tag and title_tag:
                        title = self.clean_text(title_tag.get_text())
                        link = link_tag['href']
                        if not link.startswith('http'):
                            link = 'https://news.un.org' + link
                        
                        # ç¿»è¨³
                        title_ja = self.translate_text(title)
                        
                        self.all_news.append({
                            'title': title_ja,
                            'description': '',
                            'source': 'UN News',
                            'url': link,
                            'country': 'å›½éš›',
                            'date': datetime.now().strftime('%Y-%m-%d')
                        })
                        print(f"  âœ“ {title_ja[:50]}...")
                except:
                    continue
            
            print(f"  âœ… UN News: {len([n for n in self.all_news if n['source']=='UN News'])}ä»¶å–å¾—\n")
        except Exception as e:
            print(f"  âŒ UN News ã‚¨ãƒ©ãƒ¼: {str(e)}\n")
    
    def fetch_xinhua_news(self):
        """æ–°è¯ç¤¾ ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        try:
            print("ğŸ“¡ æ–°è¯ç¤¾ ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
            url = 'http://www.news.cn/english/asiapacific/index.htm'
            response = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            articles = soup.find_all('li', limit=5)
            
            count = 0
            for article in articles:
                if count >= 2:
                    break
                try:
                    link_tag = article.find('a', href=True)
                    if link_tag:
                        title = self.clean_text(link_tag.get_text())
                        if 'myanmar' in title.lower() or 'burma' in title.lower():
                            link = link_tag['href']
                            if not link.startswith('http'):
                                link = 'http://www.news.cn' + link
                            
                            # ç¿»è¨³
                            title_ja = self.translate_text(title)
                            
                            self.all_news.append({
                                'title': title_ja,
                                'description': '',
                                'source': 'æ–°è¯ç¤¾',
                                'url': link,
                                'country': 'ä¸­å›½',
                                'date': datetime.now().strftime('%Y-%m-%d')
                            })
                            print(f"  âœ“ {title_ja[:50]}...")
                            count += 1
                except:
                    continue
            
            print(f"  âœ… æ–°è¯ç¤¾: {len([n for n in self.all_news if n['source']=='æ–°è¯ç¤¾'])}ä»¶å–å¾—\n")
        except Exception as e:
            print(f"  âŒ æ–°è¯ç¤¾ ã‚¨ãƒ©ãƒ¼: {str(e)}\n")
    
    def collect_all_news(self):
        """ã™ã¹ã¦ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
        print("\n" + "="*80)
        print("ğŸŒ Myanmar News Collection START")
        print("="*80 + "\n")
        
        # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†
        self.fetch_bbc_news()
        time.sleep(2)
        
        self.fetch_reuters_news()
        time.sleep(2)
        
        self.fetch_rfa_news()
        time.sleep(2)
        
        self.fetch_aljazeera_news()
        time.sleep(2)
        
        self.fetch_irrawaddy_news()
        time.sleep(2)
        
        self.fetch_un_news()
        time.sleep(2)
        
        self.fetch_xinhua_news()
        
        print("="*80)
        print(f"ğŸ“Š åé›†å®Œäº†: åˆè¨ˆ {len(self.all_news)} ä»¶")
        print("="*80 + "\n")
    
    def select_top_10(self):
        """ãƒˆãƒƒãƒ—10ã‚’é¸æŠ"""
        print("ğŸ¯ ãƒˆãƒƒãƒ—10ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’é¸æŠä¸­...\n")
        
        # é‡è¤‡å‰Šé™¤
        unique_news = []
        seen_titles = set()
        
        for news in self.all_news:
            title_key = news['title'][:50]
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        # ã‚½ãƒ¼ã‚¹ã®å¤šæ§˜æ€§ã‚’ç¢ºä¿ã—ãªãŒã‚‰10ä»¶é¸æŠ
        selected = []
        used_sources = set()
        
        # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰æœ€å¤§2ä»¶ãšã¤
        for news in unique_news:
            source = news['source']
            source_count = len([n for n in selected if n['source'] == source])
            
            if source_count < 2 and len(selected) < 10:
                selected.append(news)
                used_sources.add(source)
        
        # 10ä»¶ã«æº€ãŸãªã„å ´åˆã¯æ®‹ã‚Šã‚’è¿½åŠ 
        for news in unique_news:
            if len(selected) >= 10:
                break
            if news not in selected:
                selected.append(news)
        
        # æœ€æ–°é †ã«ã‚½ãƒ¼ãƒˆï¼ˆæ—¥ä»˜ãŒãªã‘ã‚Œã°ãã®ã¾ã¾ï¼‰
        selected = selected[:10]
        
        print(f"âœ… ãƒˆãƒƒãƒ—10é¸æŠå®Œäº†\n")
        print("é¸æŠã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹:")
        for i, news in enumerate(selected, 1):
            print(f"  {i}. [{news['source']}] {news['title'][:60]}...")
        
        return selected
    
    def save_to_json(self, news_list):
        """JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        output = {
            'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_news': len(news_list),
            'news': news_list
        }
        
        with open('myanmar_news_top10.json', 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ä¿å­˜å®Œäº†: myanmar_news_top10.json")
        print(f"   æœ€çµ‚æ›´æ–°: {output['last_updated']}")
        print(f"   ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {output['total_news']}ä»¶")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    collector = MyanmarNewsCollector()
    
    # ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
    collector.collect_all_news()
    
    # ãƒˆãƒƒãƒ—10é¸æŠ
    top_10 = collector.select_top_10()
    
    # JSONä¿å­˜
    collector.save_to_json(top_10)
    
    print("\n" + "="*80)
    print("âœ¨ ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ!")
    print("="*80)

if __name__ == "__main__":
    main()