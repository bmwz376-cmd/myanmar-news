#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒŸãƒ£ãƒ³ãƒãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆ
NewsAPI + Web Scraping ã‚’çµ±åˆã—ã€é‡è¤‡ã‚’æ’é™¤ã—ã¦10ä»¶ã«å³é¸
"""

import requests
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator
import json
from datetime import datetime, timezone, timedelta
import time
import os
from difflib import SequenceMatcher

# æ—¥æœ¬æ™‚é–“ï¼ˆJSTï¼‰
JST = timezone(timedelta(hours=9))

# NewsAPIè¨­å®š
NEWSAPI_KEY = os.environ.get('NEWSAPI_KEY', '54eacbe065dd4677964af80b966be5a2')
NEWSAPI_BASE_URL = 'https://newsapi.org/v2/everything'

# ç¿»è¨³è¨­å®š
translator_ja = GoogleTranslator(source='auto', target='ja')

# ãƒ˜ãƒƒãƒ€ãƒ¼è¨­å®š
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def similarity(a, b):
    """2ã¤ã®æ–‡å­—åˆ—ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ï¼ˆ0.0ã€œ1.0ï¼‰"""
    return SequenceMatcher(None, a.lower(), b.lower()).ratio()

def is_duplicate(news_item, news_list, threshold=0.85):
    """é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã®é¡ä¼¼åº¦ã§åˆ¤å®šï¼‰"""
    for existing in news_list:
        if similarity(news_item['title_en'], existing['title_en']) > threshold:
            return True
    return False

def translate_text(text, max_length=5000):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ï¼ˆé•·æ–‡å¯¾å¿œï¼‰"""
    if not text:
        return ""
    try:
        # é•·ã™ãã‚‹å ´åˆã¯åˆ†å‰²
        if len(text) > max_length:
            text = text[:max_length]
        return translator_ja.translate(text)
    except Exception as e:
        print(f"  âš ï¸ ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
        return text

# ==================== NewsAPIåé›† ====================

def fetch_from_newsapi():
    """NewsAPIã‹ã‚‰å„å›½ã®ãƒŸãƒ£ãƒ³ãƒãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†"""
    print("\nğŸŒ NewsAPIã‹ã‚‰åé›†ä¸­...")
    all_news = []
    
    # æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆå›½ãƒ»åœ°åŸŸåˆ¥ï¼‰
    queries = [
        {'q': 'Myanmar OR Burma', 'language': 'en', 'country_tag': 'ğŸŒå›½éš›'},
        {'q': 'Myanmar AND (military OR coup OR crisis)', 'language': 'en', 'country_tag': 'ğŸ‡²ğŸ‡²ãƒŸãƒ£ãƒ³ãƒãƒ¼'},
        {'q': 'Myanmar AND Thailand', 'language': 'en', 'country_tag': 'ğŸ‡¹ğŸ‡­ã‚¿ã‚¤'},
        {'q': 'Myanmar', 'language': 'en', 'sources': 'the-washington-post,reuters,bbc-news,cnn', 'country_tag': 'ğŸ‡ºğŸ‡¸ã‚¢ãƒ¡ãƒªã‚«'},
        {'q': 'ãƒŸãƒ£ãƒ³ãƒãƒ¼', 'language': 'ja', 'country_tag': 'ğŸ‡¯ğŸ‡µæ—¥æœ¬'},
        {'q': 'ë¯¸ì–€ë§ˆ', 'language': 'ko', 'country_tag': 'ğŸ‡°ğŸ‡·éŸ“å›½'},
        {'q': 'ç¼…ç”¸', 'language': 'zh', 'country_tag': 'ğŸ‡¨ğŸ‡³ä¸­å›½'},
    ]
    
    for query_config in queries:
        try:
            # APIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
            params = {
                'apiKey': NEWSAPI_KEY,
                'q': query_config['q'],
                'language': query_config.get('language', 'en'),
                'sortBy': 'publishedAt',
                'pageSize': 5,
                'from': (datetime.now(JST) - timedelta(days=3)).strftime('%Y-%m-%d')
            }
            
            # ã‚½ãƒ¼ã‚¹æŒ‡å®šãŒã‚ã‚‹å ´åˆ
            if 'sources' in query_config:
                params['sources'] = query_config['sources']
            
            response = requests.get(NEWSAPI_BASE_URL, params=params, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                articles = data.get('articles', [])
                
                for article in articles:
                    title_en = article.get('title', '')
                    description_en = article.get('description', '') or article.get('content', '')
                    
                    if not title_en or title_en == '[Removed]':
                        continue
                    
                    # æ—¥æœ¬èªã«ç¿»è¨³
                    title_ja = translate_text(title_en)
                    summary_ja = translate_text(description_en[:500] if description_en else '')
                    
                    news_item = {
                        'title': title_ja,
                        'title_en': title_en,
                        'summary': summary_ja,
                        'url': article.get('url', ''),
                        'source': f"{query_config['country_tag']} {article.get('source', {}).get('name', 'NewsAPI')}",
                        'published_date': article.get('publishedAt', '')[:10],
                        'country_tag': query_config['country_tag']
                    }
                    
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if not is_duplicate(news_item, all_news):
                        all_news.append(news_item)
                        print(f"  âœ“ {query_config['country_tag']}: {title_ja[:50]}...")
                
                time.sleep(0.5)  # APIåˆ¶é™å¯¾ç­–
                
            else:
                print(f"  âš ï¸ NewsAPI ã‚¨ãƒ©ãƒ¼ ({query_config['country_tag']}): {response.status_code}")
                
        except Exception as e:
            print(f"  âŒ NewsAPI åé›†ã‚¨ãƒ©ãƒ¼ ({query_config['country_tag']}): {e}")
    
    print(f"  â†’ NewsAPI: {len(all_news)}ä»¶åé›†")
    return all_news

# ==================== Web Scrapingåé›† ====================

def fetch_bbc_news():
    """BBC Myanmar ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    print("\nğŸ“° BBC Myanmar ã‹ã‚‰åé›†ä¸­...")
    news_list = []
    
    try:
        url = 'https://www.bbc.com/news/topics/c8nq32jw5r7t'
        response = requests.get(url, headers=HEADERS, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # BBC ã®è¨˜äº‹ã‚’æ¤œç´¢ï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œï¼‰
            articles = soup.select('div[data-testid="edinburgh-card"]')
            
            if not articles:
                articles = soup.select('article')
            
            for article in articles[:5]:
                try:
                    # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                    title_tag = article.find('h2') or article.find('h3')
                    if not title_tag:
                        continue
                    
                    title_en = title_tag.get_text(strip=True)
                    
                    # URLå–å¾—
                    link_tag = article.find('a', href=True)
                    if not link_tag:
                        continue
                    
                    article_url = link_tag['href']
                    if not article_url.startswith('http'):
                        article_url = 'https://www.bbc.com' + article_url
                    
                    # æ¦‚è¦å–å¾—
                    summary_tag = article.find('p')
                    summary_en = summary_tag.get_text(strip=True) if summary_tag else ''
                    
                    # ç¿»è¨³
                    title_ja = translate_text(title_en)
                    summary_ja = translate_text(summary_en) if summary_en else title_ja
                    
                    news_item = {
                        'title': title_ja,
                        'title_en': title_en,
                        'summary': summary_ja,
                        'url': article_url,
                        'source': 'ğŸŒå›½éš› BBC News',
                        'published_date': datetime.now(JST).strftime('%Y-%m-%d'),
                        'country_tag': 'ğŸŒå›½éš›'
                    }
                    
                    news_list.append(news_item)
                    print(f"  âœ“ BBC: {title_ja[:50]}...")
                    
                except Exception as e:
                    continue
                    
    except Exception as e:
        print(f"  âŒ BBC åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"  â†’ BBC: {len(news_list)}ä»¶åé›†")
    return news_list

def fetch_reuters_news():
    """Reuters Myanmar ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    print("\nğŸ“° Reuters Myanmar ã‹ã‚‰åé›†ä¸­...")
    news_list = []
    
    try:
        url = 'https://www.reuters.com/world/asia-pacific/myanmar/'
        response = requests.get(url, headers=HEADERS, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Reuters ã®è¨˜äº‹ã‚’æ¤œç´¢
            articles = soup.select('li[data-testid="MediaStoryCard"]')
            
            if not articles:
                articles = soup.select('article')
            
            for article in articles[:5]:
                try:
                    # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                    title_tag = article.find('h3') or article.find('h2')
                    if not title_tag:
                        continue
                    
                    title_en = title_tag.get_text(strip=True)
                    
                    # URLå–å¾—
                    link_tag = article.find('a', href=True)
                    if not link_tag:
                        continue
                    
                    article_url = link_tag['href']
                    if not article_url.startswith('http'):
                        article_url = 'https://www.reuters.com' + article_url
                    
                    # æ¦‚è¦å–å¾—
                    summary_tag = article.find('p')
                    summary_en = summary_tag.get_text(strip=True) if summary_tag else ''
                    
                    # ç¿»è¨³
                    title_ja = translate_text(title_en)
                    summary_ja = translate_text(summary_en) if summary_en else title_ja
                    
                    news_item = {
                        'title': title_ja,
                        'title_en': title_en,
                        'summary': summary_ja,
                        'url': article_url,
                        'source': 'ğŸŒå›½éš› Reuters',
                        'published_date': datetime.now(JST).strftime('%Y-%m-%d'),
                        'country_tag': 'ğŸŒå›½éš›'
                    }
                    
                    news_list.append(news_item)
                    print(f"  âœ“ Reuters: {title_ja[:50]}...")
                    
                except Exception as e:
                    continue
                    
    except Exception as e:
        print(f"  âŒ Reuters åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"  â†’ Reuters: {len(news_list)}ä»¶åé›†")
    return news_list

def fetch_rfa_news():
    """Radio Free Asia Myanmar ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
    print("\nğŸ“° Radio Free Asia Myanmar ã‹ã‚‰åé›†ä¸­...")
    news_list = []
    
    try:
        url = 'https://www.rfa.org/english/news/myanmar'
        response = requests.get(url, headers=HEADERS, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # RFA ã®è¨˜äº‹ã‚’æ¤œç´¢
            articles = soup.select('div.sectionteaser')
            
            if not articles:
                articles = soup.select('article')
            
            for article in articles[:5]:
                try:
                    # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                    title_tag = article.find('h2') or article.find('h3') or article.find('a')
                    if not title_tag:
                        continue
                    
                    title_en = title_tag.get_text(strip=True)
                    
                    # URLå–å¾—
                    link_tag = article.find('a', href=True)
                    if not link_tag:
                        continue
                    
                    article_url = link_tag['href']
                    if not article_url.startswith('http'):
                        article_url = 'https://www.rfa.org' + article_url
                    
                    # æ¦‚è¦å–å¾—
                    summary_tag = article.find('p')
                    summary_en = summary_tag.get_text(strip=True) if summary_tag else ''
                    
                    # ç¿»è¨³
                    title_ja = translate_text(title_en)
                    summary_ja = translate_text(summary_en) if summary_en else title_ja
                    
                    news_item = {
                        'title': title_ja,
                        'title_en': title_en,
                        'summary': summary_ja,
                        'url': article_url,
                        'source': 'ğŸ‡ºğŸ‡¸ã‚¢ãƒ¡ãƒªã‚« Radio Free Asia',
                        'published_date': datetime.now(JST).strftime('%Y-%m-%d'),
                        'country_tag': 'ğŸ‡ºğŸ‡¸ã‚¢ãƒ¡ãƒªã‚«'
                    }
                    
                    news_list.append(news_item)
                    print(f"  âœ“ RFA: {title_ja[:50]}...")
                    
                except Exception as e:
                    continue
                    
    except Exception as e:
        print(f"  âŒ RFA åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"  â†’ RFA: {len(news_list)}ä»¶åé›†")
    return news_list

def fetch_irrawaddy_news():
    """The Irrawaddy ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆãƒŸãƒ£ãƒ³ãƒãƒ¼å›½å†…ãƒ¡ãƒ‡ã‚£ã‚¢ï¼‰"""
    print("\nğŸ“° The Irrawaddy ã‹ã‚‰åé›†ä¸­...")
    news_list = []
    
    try:
        url = 'https://www.irrawaddy.com/'
        response = requests.get(url, headers=HEADERS, timeout=15)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # è¨˜äº‹ã‚’æ¤œç´¢
            articles = soup.select('article')
            
            for article in articles[:5]:
                try:
                    # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
                    title_tag = article.find('h2') or article.find('h3')
                    if not title_tag:
                        continue
                    
                    title_en = title_tag.get_text(strip=True)
                    
                    # URLå–å¾—
                    link_tag = title_tag.find('a', href=True) or article.find('a', href=True)
                    if not link_tag:
                        continue
                    
                    article_url = link_tag['href']
                    if not article_url.startswith('http'):
                        article_url = 'https://www.irrawaddy.com' + article_url
                    
                    # æ¦‚è¦å–å¾—
                    summary_tag = article.find('p')
                    summary_en = summary_tag.get_text(strip=True) if summary_tag else ''
                    
                    # ç¿»è¨³
                    title_ja = translate_text(title_en)
                    summary_ja = translate_text(summary_en) if summary_en else title_ja
                    
                    news_item = {
                        'title': title_ja,
                        'title_en': title_en,
                        'summary': summary_ja,
                        'url': article_url,
                        'source': 'ğŸ‡²ğŸ‡²ãƒŸãƒ£ãƒ³ãƒãƒ¼ The Irrawaddy',
                        'published_date': datetime.now(JST).strftime('%Y-%m-%d'),
                        'country_tag': 'ğŸ‡²ğŸ‡²ãƒŸãƒ£ãƒ³ãƒãƒ¼'
                    }
                    
                    news_list.append(news_item)
                    print(f"  âœ“ Irrawaddy: {title_ja[:50]}...")
                    
                except Exception as e:
                    continue
                    
    except Exception as e:
        print(f"  âŒ Irrawaddy åé›†ã‚¨ãƒ©ãƒ¼: {e}")
    
    print(f"  â†’ Irrawaddy: {len(news_list)}ä»¶åé›†")
    return news_list

# ==================== ãƒ¡ã‚¤ãƒ³å‡¦ç† ====================

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼šå…¨ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ã—ã¦çµ±åˆ"""
    print("=" * 60)
    print("ğŸ“° ãƒŸãƒ£ãƒ³ãƒãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç‰ˆã‚’é–‹å§‹...")
    print("=" * 60)
    
    all_news = []
    
    # NewsAPI ã‹ã‚‰åé›†
    newsapi_articles = fetch_from_newsapi()
    all_news.extend(newsapi_articles)
    
    # Web Scraping ã‹ã‚‰åé›†
    bbc_articles = fetch_bbc_news()
    reuters_articles = fetch_reuters_news()
    rfa_articles = fetch_rfa_news()
    irrawaddy_articles = fetch_irrawaddy_news()
    
    # çµ±åˆï¼ˆé‡è¤‡æ’é™¤ã—ãªãŒã‚‰ï¼‰
    for article in (bbc_articles + reuters_articles + rfa_articles + irrawaddy_articles):
        if not is_duplicate(article, all_news, threshold=0.85):
            all_news.append(article)
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š åˆè¨ˆ: {len(all_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†")
    print("=" * 60)
    
    # æœ€æ–°é †ã«ã‚½ãƒ¼ãƒˆ
    all_news.sort(key=lambda x: x.get('published_date', ''), reverse=True)
    
    # ä¸Šä½10ä»¶ã‚’å³é¸
    top_10_news = all_news[:10]
    
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    output_file = 'myanmar_news_top10.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(top_10_news, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… å®Œäº†ï¼{len(top_10_news)}ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
    print("=" * 60)
    
    # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    country_counts = {}
    for news in top_10_news:
        tag = news.get('country_tag', 'ä¸æ˜')
        country_counts[tag] = country_counts.get(tag, 0) + 1
    
    print("\nğŸ“Š å›½åˆ¥çµ±è¨ˆ:")
    for country, count in sorted(country_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"  {country}: {count}ä»¶")
    print("=" * 60)

if __name__ == '__main__':
    main()
